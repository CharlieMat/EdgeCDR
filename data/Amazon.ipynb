{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from preprocess import setup_path, DOMAINS, N_CORES, ROOT_PATH\n",
    "data_path = ROOT_PATH + \"public/amz_rating/\"\n",
    "target_path = ROOT_PATH + \"public/fedct/domain_data/\"\n",
    "setup_path(target_path, is_dir = True)\n",
    "domains = DOMAINS\n",
    "n_cores = N_CORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# domains = [\"Books\", \"Clothing_Shoes_and_Jewelry\", \"Home_and_Kitchen\", \n",
    "#            \"Electronics\", \"Sports_and_Outdoors\", \"Tools_and_Home_Improvement\", \n",
    "#            \"Movies_and_TV\", \"Toys_and_Games\", \"Automotive\", \"Pet_Supplies\", \n",
    "#            \"Kindle_Store\", \"Office_Products\", \"Patio_Lawn_and_Garden\", \n",
    "#            \"Grocery_and_Gourmet_Food\", \"CDs_and_Vinyl\", \"Video_Games\"]\n",
    "\n",
    "# n_cores = {\n",
    "#             \"Books\": [20,30],\n",
    "#             \"Kindle_Store\": [5,5],\n",
    "#             \"Home_and_Kitchen\": [10,10],\n",
    "#             \"Grocery_and_Gourmet_Food\": [5,5],\n",
    "#             \"Clothing_Shoes_and_Jewelry\": [10,10],\n",
    "#             \"Office_Products\": [5,5],\n",
    "#             \"Pet_Supplies\": [5,5],\n",
    "#             \"Tools_and_Home_Improvement\": [5,5],\n",
    "#             \"Electronics\": [10,10],\n",
    "#             \"Automotive\": [5,5],\n",
    "#             \"Sports_and_Outdoors\": [5,5],\n",
    "#             \"Patio_Lawn_and_Garden\": [5,5],\n",
    "#             \"Toys_and_Games\": [5,5],\n",
    "#             \"Movies_and_TV\": [10,5],\n",
    "#             \"CDs_and_Vinyl\": [5,5],\n",
    "#             \"Video_Games\": [5,5]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from preprocess import *\n",
    "\n",
    "def filter_items_with_meta(df, meta_file_path):\n",
    "    items = {iid: False for iid in df['ItemID'].unique()}\n",
    "    item_meta = {}\n",
    "    with open(meta_file_path, 'r') as fin:\n",
    "        fin.readline()\n",
    "        for i,line in tqdm(enumerate(fin)):\n",
    "            meta_info = line.strip().split(\"\\t\")\n",
    "            item_id = meta_info[0]\n",
    "            if item_id in items:\n",
    "                item_meta[item_id] = meta_info\n",
    "                del items[item_id]\n",
    "                if len(items) == 0:\n",
    "                    break\n",
    "    print(\"Item meta info of items in data set:\")\n",
    "    print(f\"Found: {len(item_meta)}\")\n",
    "    print(f\"Missing: {len(items)}\")\n",
    "    selected_rows = [True] * len(df)\n",
    "    for i,iid in enumerate(df[\"ItemID\"]):\n",
    "        if iid in items:\n",
    "            selected_rows[i] = False\n",
    "    df = df[selected_rows]\n",
    "    return item_meta, df\n",
    "\n",
    "def multi_domain_multicore(domain_n_cores, target_dir):\n",
    "    for domain, n_cores in domain_n_cores.items():\n",
    "        print(domain)\n",
    "        print(n_cores)\n",
    "        df = pd.read_table(data_path + domain + \".csv\", sep=\",\", \n",
    "                           names = [\"ItemID\", \"UserID\", \"Response\", \"Timestamp\"])\n",
    "        multicore_data = run_multicore_asymetric(df[[\"UserID\", \"ItemID\", \"Response\", \"Timestamp\"]], \n",
    "                                                 n_core_user = n_cores[0], \n",
    "                                                 n_core_item = n_cores[1])\n",
    "        multicore_data = run_multicore_asymetric(multicore_data, n_cores[0], n_cores[1])\n",
    "        multicore_data.to_csv(target_dir + \"multicore_data/\" + domain + \".tsv\", sep = '\\t', index = False)\n",
    "        n_user, n_item = len(multicore_data.UserID.unique()), len(multicore_data.ItemID.unique())\n",
    "        print(f\"#user: {n_user}\")\n",
    "        print(f\"#item: {n_item}\")\n",
    "        print(f\"#record: {len(multicore_data)}\")\n",
    "        print(f\"sparsity: {1.0 - len(multicore_data) / (n_user * n_item)}\")\n",
    "        # item meta\n",
    "        items = {iid: iid for iid in multicore_data['ItemID'].unique()}\n",
    "        item_meta_df = pd.DataFrame.from_dict(items, orient = \"index\", columns = [\"ItemID\"])\n",
    "        item_meta_df = item_meta_df.reset_index(drop = True)\n",
    "        item_meta_df.to_csv(target_dir + \"meta_data/\" + domain + \"_item.meta\", sep = '\\t', index = False)\n",
    "        # item vocabulary\n",
    "        build_vocab(item_meta_df, target_dir + \"meta_data/\" + domain + \"_item_fields.vocab\", [\"ItemID\"])\n",
    "        # item feature description\n",
    "        item_fields_meta = pd.DataFrame({\n",
    "            \"field_name\": [\"ItemID\"], \n",
    "            \"field_type\": [\"nominal\"], \n",
    "            \"value_type\": [\"str\"], \n",
    "            \"field_enc\": [\"v2id\"], \n",
    "            \"vocab_key\": [\"ItemID\"]})\n",
    "        item_fields_meta.to_csv(target_dir + \"meta_data/\" + domain + \"_item_fields.meta\", \n",
    "                                sep = '\\t', index = False)\n",
    "        # user meta\n",
    "        users = {uid: uid for uid in multicore_data['UserID'].unique()}\n",
    "        user_meta_df = pd.DataFrame.from_dict(users, orient = \"index\", columns = [\"UserID\"])\n",
    "        user_meta_df = user_meta_df.reset_index(drop = True)\n",
    "        user_meta_df.to_csv(target_dir + \"meta_data/\" + domain + \"_user.meta\", sep = '\\t', index = False)\n",
    "        # user vocabulary\n",
    "        build_vocab(user_meta_df, target_dir + \"meta_data/\" + domain + \"_user_fields.vocab\", [\"UserID\"])\n",
    "        # user feature description\n",
    "        user_fields_meta = pd.DataFrame({\"field_name\": [\"UserID\"], \n",
    "                                  \"field_type\": [\"nominal\"], \n",
    "                                  \"value_type\": [\"str\"], \n",
    "                                  \"field_enc\": [\"v2id\"], \n",
    "                                  \"vocab_key\": [\"UserID\"]})\n",
    "        user_fields_meta.to_csv(target_dir + \"meta_data/\" + domain + \"_user_fields.meta\", \n",
    "                                sep = '\\t', index = False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setup_path(target_path + \"meta_data/\", is_dir = True)\n",
    "setup_path(target_path + \"multicore_data/\", is_dir = True)\n",
    "multi_domain_multicore(n_cores, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-Domain Statistics\n",
    "\n",
    "**Note**: Run the first section before running this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "def get_cross_domain_frequencies(domains, target_dir):\n",
    "    user_frequencies = {}\n",
    "    for i,domain in enumerate(domains):\n",
    "        print(domain)\n",
    "        df = pd.read_csv(target_dir + \"multicore_data/\" + domain + \".tsv\", sep = '\\t')\n",
    "        UC = df['UserID'].value_counts()\n",
    "        uids = list(UC.index)\n",
    "        counts = list(UC.values)\n",
    "        for j,u in tqdm(enumerate(uids)):\n",
    "            c = counts[j]\n",
    "            if u not in user_frequencies:\n",
    "                user_frequencies[u] = [0] * len(domains)\n",
    "            user_frequencies[u][i] += c\n",
    "    return user_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UF = get_cross_domain_frequencies(list(n_cores.keys()), target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(user_freq, domains):\n",
    "    M = [[0]*len(domains) for _ in range(len(domains))]\n",
    "    for i in tqdm(range(len(domains))):\n",
    "        for j in range(len(domains)):\n",
    "            for k,freq in user_freq.items():\n",
    "                if freq[i] > 0 and freq[j] > 0:\n",
    "                    M[i][j] += freq[j]\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_domain_matrix(user_freq, domains):\n",
    "    M = [[0]*len(domains) for _ in range(len(domains))]\n",
    "    for i in tqdm(range(len(domains))):\n",
    "        for j in range(len(domains)):\n",
    "            for k,freq in user_freq.items():\n",
    "                if freq[i] > 0 and freq[j] > 0:\n",
    "                    M[i][j] += 1\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = get_confusion_matrix(UF, list(n_cores.keys()))\n",
    "M = get_cross_domain_matrix(UF, list(n_cores.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_heatmap(x_labels, y_labels, matrix, title = \"confusion matrix\", include_diagonal = True):\n",
    "    assert len(matrix) == len(y_labels) and len(matrix[0]) == len(x_labels)\n",
    "    fig, ax = plt.subplots(figsize = (len(x_labels),len(y_labels)))\n",
    "    if not include_diagonal:\n",
    "        im = ax.imshow([[np.log(matrix[i][j]) if i!=j else 0 for j in range(len(x_labels))] for i in range(len(y_labels))])\n",
    "    else:\n",
    "        im = ax.imshow([[np.log(matrix[i][j]) for j in range(len(x_labels))] for i in range(len(y_labels))])\n",
    "\n",
    "    ax.set_xticks(np.arange(len(x_labels)))\n",
    "    ax.set_yticks(np.arange(len(y_labels)))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    for i in range(len(y_labels)):\n",
    "        for j in range(len(x_labels)):\n",
    "            text = ax.text(j, i, matrix[i][j], fontsize = 'small',\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "            \n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "D = list(n_cores.keys())\n",
    "plot_heatmap(D,D,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_prob = []\n",
    "for i,D1 in enumerate(D):\n",
    "    from_d_sharing = [float(M[i][j])/M[i][i] for j,D2 in enumerate(D) if i != j]\n",
    "    sharing_prob.append(np.mean(from_d_sharing))\n",
    "print(\"Average common user prob: \", np.mean(sharing_prob))\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.barh(D,sharing_prob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_unique_prop(user_freq, domains):\n",
    "    unique_freq = {d: 0. for d in domains}\n",
    "    domain_freq = {d: 0. for d in domains}\n",
    "    for k,freq in tqdm(user_freq.items()):\n",
    "        sum_freq = sum(freq)\n",
    "        for i,domain in enumerate(domains):\n",
    "            if freq[i] > 0:\n",
    "                domain_freq[domain] += 1\n",
    "                if freq[i] == sum_freq:\n",
    "                    unique_freq[domain] += 1\n",
    "    return {d: unique_freq[d]/domain_freq[d] for d in domains}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_prop = get_domain_unique_prop(UF, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "V = list(unique_prop.values())\n",
    "print(\"Average unique user prob: \", np.mean(V))\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.barh(list(unique_prop.keys()),V)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Holdout Cross-domain Cold-start Users\n",
    "\n",
    "**Note**: Run the first cell and section 2 before running this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_prob = 0.1 # x% of the users\n",
    "min_holdout_freq = 40 # minimum cross-domain history length\n",
    "max_holdout_bound = 0.2 # hold out y% of the record in cross-domain history\n",
    "\n",
    "holdout_candidates = {d: [] for d in D}\n",
    "holdout_number = {d: holdout_prob * M[i][i] for i,d in enumerate(D)}\n",
    "for uid, freq in tqdm(UF.items()):\n",
    "    # all user frequencies across domains\n",
    "    total_freq = sum(freq)\n",
    "    if total_freq < min_holdout_freq:\n",
    "        continue\n",
    "    # {domain_name: user's frequency on domain_name}\n",
    "    valid_domain_freq = {D[i]:f for i,f in enumerate(freq) if f > 0}\n",
    "    # maximum frequency to holdout\n",
    "    available_freq = max_holdout_bound * total_freq\n",
    "    # holdout when \n",
    "    # * There is still available frequency\n",
    "    # * There is still less than 10% users held out in the domain\n",
    "    for d,f in valid_domain_freq.items():\n",
    "        if f < available_freq and len(holdout_candidates[d]) < holdout_number[d]:\n",
    "            holdout_candidates[d].append(uid)\n",
    "            available_freq -= f\n",
    "print(\"#user candidates to holdout\")\n",
    "print({d: len(candidates) for d,candidates in holdout_candidates.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cold holdout will split users by 9-1:\n",
    "train: random 90% of user\n",
    "test: random 10% of user\n",
    "'''\n",
    "from preprocess import move_user_data\n",
    "def recheck_exist(trainset, testset, field_name):\n",
    "        print(\"Move unseen \" + field_name + \" from test to train, this may also move users in val to train\")\n",
    "        V = {v:1 for v in trainset[field_name].unique()} # updated ids in train\n",
    "        test_user_hist = {} # {uid: [row_id]}\n",
    "        moving_user = {} # [uid], set of users to move from test/val to train\n",
    "        pos = 0\n",
    "        for u,i in zip(testset[\"UserID\"], testset[field_name]):\n",
    "            if u not in test_user_hist:\n",
    "                test_user_hist[u] = list()\n",
    "            test_user_hist[u].append(pos)\n",
    "            pos += 1\n",
    "            if i not in V:\n",
    "                moving_user[u] = 1\n",
    "        moving_user = list(moving_user.keys())\n",
    "        print(\"Test --> Train\")\n",
    "        trainset, testset = move_user_data(from_df = testset, to_df = trainset, \n",
    "                                           moving_user = moving_user, \n",
    "                                           user_hist = test_user_hist, field_name = field_name)\n",
    "        return trainset, testset\n",
    "        \n",
    "def holdout_cross_domain_cold_start(target_dir, holdout_candidates):\n",
    "    for domain,uids in holdout_candidates.items():\n",
    "        selected = {u: True for u in uids}\n",
    "        # data\n",
    "        df = pd.read_csv(target_dir + \"multicore_data/\" + domain + \".tsv\", sep = '\\t')\n",
    "        # Build user history\n",
    "        user_hist = {}\n",
    "        for pos,row in tqdm(enumerate(df.values)):\n",
    "            u, *record = row\n",
    "            if u not in user_hist:\n",
    "                user_hist[u] = list()\n",
    "            user_hist[u].append(pos)\n",
    "        # holdout cold-start\n",
    "        print(\"Holdout cold-start user histories\")\n",
    "        test_indices = df[\"UserID\"]==-1\n",
    "        for u,H in tqdm(user_hist.items()):\n",
    "            if u in selected:\n",
    "                test_indices.iloc[H] = True\n",
    "        testset = df[test_indices]\n",
    "        trainset = df[~test_indices]\n",
    "        # recheck exist\n",
    "        trainset = trainset.reset_index(drop = True)\n",
    "        testset = testset.reset_index(drop = True)\n",
    "        trainset, testset = recheck_exist(trainset, testset, field_name = \"ItemID\")\n",
    "        # save data\n",
    "        trainset.to_csv(target_dir + \"tsv_data/\" + domain + \"_local.tsv\", sep = '\\t', index = False)\n",
    "        testset.to_csv(target_dir + \"tsv_data/\" + domain + \"_test_cold.tsv\", sep = '\\t', index = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setup_path(target_path + \"tsv_data/\", is_dir = True)\n",
    "holdout_cross_domain_cold_start(target_path, holdout_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Holdout Each Domain\n",
    "\n",
    "Warm or leave-one-out\n",
    "\n",
    "**Note**: Run the first cell before running this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import holdout_data_sequential, recheck_exist\n",
    "import pandas as pd\n",
    "\n",
    "def holdout_single_domain_data(target_dir, domains, holdout_type = 'leave_one_out'):\n",
    "    for domain in domains:\n",
    "        print(domain, \": \")\n",
    "        # data\n",
    "        df = pd.read_csv(target_dir + \"tsv_data/\" + domain + \"_local.tsv\", sep = '\\t')\n",
    "        # holdout\n",
    "        df = df.sort_values(by=['UserID','Timestamp'])\n",
    "        trainset, valset, testset = holdout_data_sequential(df, holdout_type, min_hist_len = 10)\n",
    "        trainset = trainset.reset_index(drop = True)\n",
    "        valset = valset.reset_index(drop = True)\n",
    "        testset = testset.reset_index(drop = True)\n",
    "        trainset, valset, testset = recheck_exist(trainset, valset, testset, field_name = \"ItemID\")\n",
    "        # save\n",
    "        trainset.to_csv(target_dir + \"tsv_data/\" + domain + \"_train.tsv\", sep = '\\t', index = False)\n",
    "        valset.to_csv(target_dir + \"tsv_data/\" + domain + \"_val.tsv\", sep = '\\t', index = False)\n",
    "        testset.to_csv(target_dir + \"tsv_data/\" + domain + \"_test.tsv\", sep = '\\t', index = False)\n",
    "        print(\"#User: \", len(trainset[\"UserID\"].unique()), len(valset[\"UserID\"].unique()), len(testset[\"UserID\"].unique()))\n",
    "        print(\"#Item: \", len(trainset[\"ItemID\"].unique()), len(valset[\"ItemID\"].unique()), len(testset[\"ItemID\"].unique()))\n",
    "        print(\"#Record: \", len(trainset), len(valset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holdout_type = \"leave_one_out\" # \"warm\" \"leave_one_out\"\n",
    "holdout_single_domain_data(target_path, list(n_cores.keys()), holdout_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedCT",
   "language": "python",
   "name": "fedct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
