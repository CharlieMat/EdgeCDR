{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782515a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from data.preprocess import setup_path, ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0448122-f852-4e96-b637-27a638fe40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "def get_arg(textline, field):\n",
    "    return eval(textline[textline.index(field+'='):textline.find(',',textline.index(field+'='),-1)].split('=')[1])\n",
    "\n",
    "def extract_results(log_root_path, customized_args = [], file_name_identifier = \"train_and_eval\"):\n",
    "    result_dict = {}\n",
    "    for j,file in tqdm(enumerate(os.listdir(log_root_path))):\n",
    "        if file_name_identifier in file:\n",
    "            print(file)\n",
    "            args = None\n",
    "            model_name = \"\"\n",
    "            results = []\n",
    "            found = 0\n",
    "            with open(os.path.join(log_root_path, file), 'r') as fin:\n",
    "                for i,line in enumerate(fin):\n",
    "                    if i == 0:\n",
    "                        model_name = get_arg(line, 'model')\n",
    "                    if i == 1:\n",
    "                        args = line.strip()[10:-1]\n",
    "                    elif \"Test set performance\" in line:\n",
    "                        found = 2\n",
    "                    elif found > 0:\n",
    "                        found -= 1\n",
    "                        if found == 0:\n",
    "                            results.append(eval(line))\n",
    "            if len(results) > 0:\n",
    "                args += ','\n",
    "                result_dict[j] = {'args': args}\n",
    "                result_dict[j]['model_name'] = model_name\n",
    "                for k in customized_args:\n",
    "                    try:\n",
    "                        result_dict[j][k] = get_arg(args, k)\n",
    "                    except:\n",
    "                        result_dict[j][k] = 'NaN'\n",
    "                results = {k:[result[k] for result in results] for k in results[0].keys()}\n",
    "                for k,v in results.items():\n",
    "                    result_dict[j][k] = v\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ddb62",
   "metadata": {},
   "source": [
    "## 1. Extract Results from Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6fbb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from utils import extract_results, get_args\n",
    "log_path = ROOT_PATH + \"experiments/fedct/env_logs/\"\n",
    "control_args = ['lr', 'l2_coef', 'loss']\n",
    "raw_results = extract_results(log_path, control_args, \"fedct_train_and_eval\")\n",
    "print(len(raw_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80039a08-3972-41d4-b63a-7646021132e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for idx, R in raw_results.items():\n",
    "    updated_results = {}\n",
    "    for k,v in R.items():\n",
    "        updated_results[k] = v\n",
    "        if k == 'args':\n",
    "            updated_results['domain'] = get_arg(R['args'], 'reader_path')[:-4].split('/')[-1][15:]\n",
    "    results[idx] = updated_results\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "metrics = [f\"{m_name}@{i}\" for i in [1,5,10,20,50] for m_name in ['HR', 'RECALL', 'P', 'F1', 'NDCG']] + [\"AUC\", \"MR\", \"MRR\"]\n",
    "for m_name in metrics:\n",
    "    for k, res_dict in results.items():\n",
    "        if m_name in res_dict:\n",
    "            res_dict[m_name] = np.mean(res_dict[m_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221b65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results, orient = 'index')\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "target_path = 'pretrain/'\n",
    "setup_path(target_path, is_dir = True)\n",
    "result_file_path = target_path + \"env_models_performances.csv\"\n",
    "df.to_csv(result_file_path, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067c201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedCT",
   "language": "python",
   "name": "fedct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
